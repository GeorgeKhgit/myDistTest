(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{128:function(e,t,o){"use strict";o.r(t),t.default={benefits:"ADVANTAGES",opportunities:"OPPORTUNITIES",cases:"CASES",contact:"CONTACT US",en:"EN",ru:"RU",headline:"DATAGRAM",headlineDesc:"Low-code data engineering platform",headlineDesc2:'Master <span class="roboto">Data Lake</span> in a quick and easy way',downloadDemo:"DOWNLOAD DATAGRAM",technical:"TECHNICAL OVERVEIW",topStep1:"Would you like to develop analytical solutions based on Big Data quickly?",topStep1Desc:"Use a convenient and intuitive visual development tool",topStep2:"Do you find there are insufficient Big Data specialists?",topStep2Desc:"Take advantage of your developers’ experience in ETL and SQL for visual development of data transformation processes",topStep3:"Would you like to simplify support of complex solutions based on Big Data?",topStep3Desc:"Introduce modifications in a quick and easy way with visual representation of data streams and their transformation processes",pipeLinesTitle:"Visual design of Data Pipelines and automatic code generation in Scala",pipeLinesDesc:"Use the full power of Apache Spark libraries for batch and streaming data processing without the need to write code in Scala manually. Scala code will be generated automatically using the Model Driven Architecture approach for special models.",dataDownload:"Data loading from any source in two clicks",dataDownloadDesc:"Load data based on source metadata as is, without the need to scale down to the source-to-target mapping development. Export source metadata into Apache Atlas to build Data Governance infrastructure and manage data at the organization's level.",processingTitle:"Streaming Processing",processingDesc:"Design stream data transformation processes and obtain computation results within a few milliseconds after data have appeared.",processingDescBottom:"Use Kafka and Spark Streaming to process data in streaming mode. Create and use business rules for event flows in real time through native integration with BRMS.",outBoxTitle:"The broadest range of data processing tools - out of the box:",outBoxDesc:"Machine-learning-based analysis with the use of Spark MLlib (decision trees, SVM, logistic regression, etc.);",outBoxDesc2:"Jboss Rules (Drools);",outBoxDesc3:"A broad range of relational algebra operations: join, sort, aggregation, union, selection, projections, pivot, explode arrays, sequence generation;",outBoxDesc4:"Spark SQL.",debuggingTitle:"Datagram offers handy debugging and optimization tools:",debuggingDesc:"View the contents and the structure of sources and listeners;",debuggingDesc2:"Track origins of data flow objects down to individual fields (lineage);",debuggingDesc3:"Perform partial transformation and review intermediary results;",debuggingDesc4:"Perform individual transformation steps and branches;",debuggingDesc5:"View generated application code;",debuggingDesc6:"Validate transformation automatically;",debuggingDesc7:"Spark Catalyst Optimizer support.",dataLogicTitle:"Data Pipeline with logic<br/> of any complexity",dataLogicDesc:"Integrate Artificial Intelligence/Machine Learning algorithms and business rules into data processing flows.",supportDevopsTitle:"Support of DevOps and CI/СD Pipeline",supportDevopsItemTitle1:"Repository-based change capture",supportDevopsItemTitle2:"Module assembly and installation",supportDevopsItemTitle3:"Launch of developed applications",supportDevopsItemDesc1:"Integration with version control systems",supportDevopsItemDesc2:"UNIT-test support",supportDevopsItemDesc3:"Application launch does not depend on programming environment",cardTitle1:"Anomaly detection",cardDesc1:"Real-time monitoring and anomaly detection",cardTitle2:"Analytical platform",cardDesc2:"Advanced analytics for financial organizations",cardTitle3:"Planning system",cardDesc3:"Real-time planning system for logistics",cardTitle4:"Compliance",cardDesc4:"Compliance and reporting for financial organizations",cardButton:"More details",useTitle:"Already used in their work",bigDataTitle:'Develop big data-based <br/> analytical solutions in a<br/> quick and easy way enjoying <br/><span class="roboto">DATAGRAM\'S</span> features',footerDesc:'© 2020 <span class="roboto">Datagram</span></span>',footerFeedback:"CONTACT US",caseTitle1:"Anomaly detection",caseTopTitle:"Real-time monitoring and anomaly detection",caseTopDesc:"Anomaly detection is a technique used to identify unusual patterns that do not conform to expected behavior, called outliers. It has many applications in business, from intrusion detection (identifying strange patterns in network traffic that could signal a hack) to system health monitoring (spotting a malignant tumor in an MRI scan), and from fraud detection in credit card transactions to fault detection in operating environments.",caseMiddleTitle:"Anomalies can be broadly categorized as:",caseMiddleDesc:'<ol><li><span class="roboto">Point anomalies:</span> A single instance of data is anomalous if it\'s too far off from the rest. Business use case: someone spent 1000$ for food while usually not spend more than 40$;</li><li><span class="roboto">Contextual anomalies:</span> The abnormality is context specific. This type of anomaly is common in time-series data. Business use case: A lot of login tries after end of working day;</li><li><span class="roboto">Collective anomalies:</span> A set of data instances collectively helps in detecting anomalies. Business use case: series of failed login tries during short period of time from different users followed by success try of one of them.</li></ol>',caseBottomTitle:"The main challenge is how to collect and process all necessary data in real-time to react immediately to a close threat. Moreover you have to process billions transactions and dozens Tb of system logs per day to identify events and incidents from.",caseBottomDesc:"The only way is using the latest BIG DATA technologies that are able to help to collect data from different types of structured and semi- structured data sources and process it in real-time.",caseBottomDesc12:'<span>Use DATAGRAM to build your solution </span><span class="span-gradient">faster and cheaper</span><span> without necessity to involve </span><span class="span-gradient">high qualified and expensive BIG DATA specialists</span><span> due to automatic code generation. Scala-code will be generated automatically according with best-practice.</span>',caseTitle2:"Analytical platform",caseTopTitle2:"Advanced analytics for financial organizations",caseTopDesc2:"Analytics is helping the banking industry become smarter in managing the myriad challenges it faces. While basic reporting and descriptive analytics continues to be a must-have for banks, advanced predictive and prescriptive analytics are now starting to generate powerful insights, resulting in significant business impact.",caseMiddleDesc2:"Sophisticated risk modeling presents a powerful way to understand short- and long-term profitability and capital adequacy or, in other words, chances of bank’s survival in future.",caseMiddleDesc22:"Fraud and AML/KYC analytics are helping banks stay ahead of fraudsters, terrorists and others in preventing money laundering and associated potential losses.",caseMiddleDesc23:"Consumer behavior and marketing analytics are driving sustainable competitive advantage in an era with eroding product differentiation, waning customer loyalty, and exploding volume, velocity, and variety of data.",caseBottomDesc2:"Analyze all you customer data and get incredible insights real-time with the latest BIG DATA technologies.",caseBottomDesc22:'<span>Build your solution </span><span class="span-gradient">faster and cheaper</span><span> in visual development environment without necessity to involve </span><span class="span-gradient">high qualified and expensive BIG DATA specialists</span><span> due to automatic code generation. Scala-code will be generated automatically according with best-practice.</span>',caseTitle3:"Planning system",caseTopTitle3:"Real-time planning system for logistics",caseTopDesc3:"In industries where order-fulfilment windows may be as narrow as half-an-hour, and penalties for late delivery may be substantial, responding quickly to real-time events such as traffic delays, breakdowns and last minute orders is vital.",caseTopDesc32:"Both in warehousing and in transportation, it has a great importance of how effectively can we utilize the available capacity. The rent of a warehouse will not be less if, as a result of inadequate placement of goods, we can only store as much as half of it. In the case of freight, we can not afford the same cost for a semi-loaded truck or for a car with a maximum load. Costs can be high if we do not optimally utilize the space, so it can be a simple cost-cutting and revenue-generating tool in itself if we strive for maximum utilization.",caseTopDesc33:"It can also cause serious problems if we do not have accurate and up-to-date information on our stock. The lack of information may result too large and too small inventory-neither of them is a cost-effective solution. The lack of a product can significantly slow down delivery services, while excessive volume will eventually lead to price reductions to encourage sales, so that would reduce our profit as well. Without having the right information and without the transparency of the company's business, we can lost serious profit while our costs are constantly increasing.",caseMiddleTitle3:"Nowadays the most innovative logistics companies are implementing real-time panning solutions based on BIG DATA technologies that need to respond dynamically to operational changes and unexpected events:",caseMiddleDesc3:"<ul><li>Automated allocation of work to drivers and resources;</li><li>Dynamic routing and scheduling of vehicles;</li><li>Optimized planning of deliveries, collections, trunks and multimodal operations;</li><li>Intelligent response to unplanned or last minute orders, cancellations, delays, etc.</li></ul>",caseBottomDesc3:"They choose to use latest technologies to reach comparative advantage in high competitive industry through unique solutions implementation.",caseBottomDesc32:'<span>Use DATAGRAM to build your solution </span><span class="span-gradient">faster and cheaper</span><span> without necessity to involve </span><span class="span-gradient">high qualified and expensive BIG DATA specialists</span><span> due to automatic code generation. Scala-code will be generated automatically according with best-practice.</span>',caseTitle4:"Compliance",caseTopTitle4:"Compliance and reporting for financial organizations",caseTopDesc4:"Today financial organizations have greatly expanded the scope and complexity of their activities and face an ever-changing and increasingly complex regulatory environment. Furthermore, due to the consumer credit crisis and increased emphasis on consumer protection, all regulatory agencies, investors, legislators and the general public are focused on institutions' customer practices and regulatory compliance performance like never before. Moreover, a compliance failure can result in litigation, financial penalties, regulatory constraints and reputational damage that can strategically affect an organization.",caseMiddleDesc4:'The increase in regulations has exposed several challenges within financial institutions. Today to meet regulatory requirements every financial organization has to process tremendous amount of data using the most sophisticated algorithms and methods. Now is not time of "one-size fits all" solutions. Instead, all financial organization need solution tailored to each client\'s individual business needs.',caseMiddleDesc42:"Using latest BIG DATA technologies let us resolve the problem of getting reports and analytics on time by fast processing big amount of various data. In addition, we can use advanced BIG DATA analytics tools to recognize the risks before they turn out to be harmful and considerably improve the way client risk managed.",caseBottomDesc4:'<span>Meat all compliance requirements and get your competitive advantage by build BIG DATA solution</span> <span class="span-gradient">faster and cheaper</span><span> in visual development environment without necessity to involve </span><span class="span-gradient">high qualified and expensive BIG DATA specialists</span><span> due to automatic code generation. Scala-code will be generated automatically according with best-practice.</span>',task:"The goal is",decision:"The solution is",architecture:"Solution architecture",agreement:"I give my consent to the processing of personal data",downloadDatagramTitle:"Fill in the details to get a link to download DATAGRAM.",downloadDatagramSuccess:"Thank you for your interest in DATAGRAM.<br/> We sent the download link for DATAGRAM to your e-mail address.",feedbackSuccess:"Thank you for your request. <br/> We will contact you as soon as possible.",formName:"Name *",formPosition:"Position *",formCompany:"Company *",formEmail:"E-mail *",formTel:"Phone",formComment:"Comments",formSend:"Send",flip:"Rotate the device <br> to a vertical position",hadoopTitle:"Use the most common Hadoop versions as a run-time",hadoopDesc:"DATAGRAM supports all popular Hadoop versions as a run-time. Generated spark jobs and workflows run independently from DATAGRAM during testing and production using Hadoop in-build abilities only. You can manage and monitor your data processing pipelines via advances datagram tools.",watchVideo:"PLAY VIDEO"}}}]);